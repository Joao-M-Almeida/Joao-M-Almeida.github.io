---
layout: post
title: Bias and Variance in Cross Validation strategies
---

## __TL;DR__
TESt

## Why the focus on Cross Validation ?

I have been talking with [LÃ­dia](https://github.com/lidiamcfreitas) about her Machine Learning assignments that consist of a [Kaggle](https://www.kaggle.com/) competition in the traditional format with a public and a private test dataset.
This led me to realize that Cross validation and estimating the performance of Machine Learning models is a much more complex topic than what I thought.
So I decided to explore it and to write this post as a summary of I what I learned.

## Estimating model performance


## K-Fold cross validation: the default



## Cross Validation Nested Loops


### Implementing with Scikit-learn

TODO: Add code

## Sources and Further reading:

- [Link](http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm)

__CrossValidated Answers:__

- [Link ](http://stats.stackexchange.com/questions/14474/compendium-of-cross-validation-techniques?noredirect=1&lq=1)
- [Link ](http://stats.stackexchange.com/questions/61783/variance-and-bias-in-cross-validation-why-does-leave-one-out-cv-have-higher-var/244112#244112)
- [Link ](http://stats.stackexchange.com/questions/90902/why-is-leave-one-out-cross-validation-loocv-variance-about-the-mean-estimate-f?noredirect=1&lq=1)
- [Link ](http://stats.stackexchange.com/questions/61546/optimal-number-of-folds-in-k-fold-cross-validation-is-leave-one-out-cv-always?noredirect=1&lq=1)

__Pappers:__

- [Link](http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf)
- [Link](http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf)
