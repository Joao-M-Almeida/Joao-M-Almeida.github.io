---
layout: post
title: Genetic Algorithms in one night
tldr:  Tonight I decided to implement a genetic algorithm to take care of the parameter optimization for my master thesis. I wanted to do it in one night, but ended up spending a couple of hours on the following day fixing some stuff.
---

## __TL;DR__

Tonight I decided to implement a __genetic algorithm__ to take care of the parameter optimization for my master thesis. I wanted to do it in one night, but ended up spending a couple of hours on the following day fixing some stuff. You can find the resulting code [here](https://gist.github.com/Joao-M-Almeida/06972e6a4c005a3f9c37f36e9892bcf9).

--------

## Why? How?

I have to optimize 5 models, each with more than 4 continuous parameters for my master thesis and I got tired of hand tuning them.

So I decided to spend one night trying to implement a __Genetic algorithm__ to optimize the parameters for me. Up until now I had been doing a mix of a __grid search__ and __hand picking parameter values__ to try to improve on the RMSE score of my models.

I have been working in __Python 2.7__ on my thesis so I decided to use and to make the implementation generic so it could be used in other projects.

I decided to write this post to help me organize my thoughts while implementing and to see if I would get some feedback on my implementation.

## Sources

Most of the ideas for the algorithms came from what I remembered of my Neural Networks class at TU Delft where we covered the topic and from [wikipedia](https://en.wikipedia.org/wiki/Genetic_algorithm), of course.

## What do I need from the models to optimize?

First I need to define what inputs I need from the problem to optimize.

- __Objective function__: The function that we want to minimize, I'll treat all problems as a minimization problem.
  - _input_: parameter tuple;
  - _output_: score from 0 to 1;


- __Parameter Space__ The list of valid values for each parameter, this will be a list of lists in Python. Let's consider only discrete values, as a continuous parameter can be discretized.

## What do I need to implement?

I will divide the code into two classes, the main __GeneticOptimizer__ class and the __Individual__ class, where each instance represents a possible parameter combination and it's score on the objective function.

### Individual Class

The Individual class represents each of the solutions tested on the objective function, it serves to store the parameter combination and the achieved score.

#### Attributes

An individual consists of:

- __genome__: parameter tuple;
- __score__: Objective function value if already known;

#### Methods

- __get_score__: Returns score of Individual
- __set_score__: Sets score of Individual
- __to_string__: Represents Individual in string format


### Genetic algorithm class

#### Attributes

The main Attributes that need to be stored by the __GeneticOptimizer__ class are:

- __paremeter_space__: The list of lists of valid parameter values;
- __objective_func__: The function that receives the parameter combination and outputs the score, from 0 to 1;

- __mutation_prob__: probability that a individual mutates during a generation;

- __pop_size__: Number of individuals in the population;
- __population__: List of individuals in this generation;
- __genome_set__: Set of different genomes present in the population;

- __pool__: A process pool, to allow for parallel execution of the algorithm.

#### Methods

- __generate_individual__: Sample parameter space to generate element for population;
  - _output_: One valid parameter tuple to add to test population;


- __crossover__: Mix two individuals to create next generation individual;
  - _input_: 2 parameter tuples, score of each individual;
  - _output_: New individual;

    For the crossover instead of just giving equal probability to the parameters of each individual I decided to give a __probability based on their score__:

$$P_a = \frac{s_a}{s_a + s_b}$$

- __mutate__: Change one individual to replicate the mutation mechanism that exists in nature;
  - _input_: Individual to be mutated;
  - _output_: New individual;

- __score_population__: Use the __objective function__ to find the score of every individual in the population. Here I added a small trick: as my objective functions are costly to evaluate, some take more than 30 min, I added a __dictionary of past parameters and scores__ as a form of [memoization](https://en.wikipedia.org/wiki/Memoization) of the results.

- __update_population__: The core of the algorithm, from the current population creates the next generation using different mechanisms. The process is as follows:

  1. Define the number of individuals generated by each mechanism, I set the following percentages of the population size:
    - __30% Elitists__ - the __best performing individuals__ of the previous generation
    - __50% Crossed individuals__ - Individuals generated by __crossover__ of the elitists;
    - __20% Random new individuals__ - Generated by randomly sampling the parameter space;
  2. Generate the individuals with each mechanism, and making sure __no two individuals in the population have the same genome;
  3. Look through the individuals in the new generation and with probability __mutate_prob__ mutate them.

## Results

I'm pretty happy with the results I got. It served as a refresher on genetic algorithms and it was fun to implement. I spent a bit more time on this than what I was planing, mostly because of the improvements I did after the first runs and due to the writing of this blog post.

This is a very simple post, I just wanted to write down a bit of the process. If you have any questions feel free to contact me at __jmirandadealme__ on gmail.

All the code can be found [here](https://gist.github.com/Joao-M-Almeida/06972e6a4c005a3f9c37f36e9892bcf9).
